4c4
< # and Shiqiao Du <lucidfrontier.45@gmail.com>
---
> #         Shiqiao Du <lucidfrontier.45@gmail.com>
6a7
> # More API changes: Sergei Lebedev <superbobry@gmail.com>
12,13d12
< import string
<
15c14
< from sklearn.utils import check_random_state
---
> from sklearn import cluster
17a17
>     log_multivariate_normal_density,
19c19
< from sklearn import cluster
---
> from sklearn.utils import check_random_state
21,31c21,22
< from .base import _BaseHMM, decoder_algorithms
< from .utils import normalize
< from .utils.fixes import log_multivariate_normal_density
<
< __all__ = ['GMMHMM',
<            'GaussianHMM',
<            'MultinomialHMM',
<
<            # for compatbility, but we should remove this, really.
<            'decoder_algorithms',
<            'normalize']
---
> from .base import _BaseHMM
> from .utils import iter_from_X_lengths, normalize
33c24
< NEGINF = -np.inf
---
> __all__ = ["GMMHMM", "GaussianHMM", "MultinomialHMM"]
34a26
> COVARIANCE_TYPES = frozenset(("spherical", "diag", "full", "tied"))
36,37d27
< class GaussianHMM(_BaseHMM):
<     """Hidden Markov Model with Gaussian emissions
39,41c29,30
<     Representation of a hidden Markov model probability distribution.
<     This class allows for easy evaluation of, sampling from, and
<     maximum-likelihood estimation of the parameters of a HMM.
---
> class GaussianHMM(_BaseHMM):
>     """Hidden Markov Model with Gaussian emissions.
50,57c39
<         use.  Must be one of 'spherical', 'tied', 'diag', 'full'.
<         Defaults to 'diag'.
<
<     Attributes
<     ----------
<     _covariance_type : string
<         String describing the type of covariance parameters used by
<         the model.  Must be one of 'spherical', 'tied', 'diag', 'full'.
---
>         use.  Must be one of
59,60c41,46
<     n_features : int
<         Dimensionality of the Gaussian emissions.
---
>         * "spherical" --- each state uses a single variance value that
>           applies to all features;
>         * "diag" --- each state uses a diagonal covariance matrix;
>         * "full" --- each state uses a full (i.e. unrestricted)
>           covariance matrix;
>         * "tied" --- all states use **the same** full covariance matrix.
62,63c48
<     n_components : int
<         Number of states in the model.
---
>         Defaults to "diag".
65,66c50,52
<     transmat : array, shape (`n_components`, `n_components`)
<         Matrix of transition probabilities between states.
---
>     min_covar : float
>         Floor on the diagonal of the covariance matrix to prevent
>         overfitting. Defaults to 1e-3.
68,69c54,55
<     startprob : array, shape ('n_components`,)
<         Initial state occupation distribution.
---
>     startprob_prior : array, shape (n_components, )
>         Initial state occupation prior distribution.
71,72c57,58
<     means : array, shape (`n_components`, `n_features`)
<         Mean parameters for each state.
---
>     transmat_prior : array, shape (n_components, n_components)
>         Matrix of prior transition probabilities between states.
74,81c60,62
<     covars : array
<         Covariance parameters for each state.  The shape depends on
<         ``_covariance_type``::
<
<             (`n_components`,)                   if 'spherical',
<             (`n_features`, `n_features`)              if 'tied',
<             (`n_components`, `n_features`)           if 'diag',
<             (`n_components`, `n_features`, `n_features`)  if 'full'
---
>     algorithm : string
>         Decoder algorithm. Must be one of "viterbi" or "map".
>         Defaults to "viterbi".
83,84c64,65
<     random_state: RandomState or an int seed (0 by default)
<         A random number generator instance
---
>     random_state: RandomState or an int seed
>         A random number generator instance.
87c68
<         Number of iterations to perform.
---
>         Maximum number of iterations to perform.
89,90c70,77
<     thresh : float, optional
<         Convergence threshold.
---
>     tol : float, optional
>         Convergence threshold. EM will stop if the gain in log-likelihood
>         is below this value.
>
>     verbose : bool, optional
>         When ``True`` per-iteration convergence reports are printed
>         to :data:`sys.stderr`. You can diagnose convergence via the
>         :attr:`monitor_` attribute.
95,96c82,83
<         't' for transmat, 'm' for means, and 'c' for covars.
<         Defaults to all parameters.
---
>         't' for transmat, 'm' for means and 'c' for covars. Defaults
>         to all parameters.
101,102c88,89
<         startprob, 't' for transmat, 'm' for means, and 'c' for
<         covars.  Defaults to all parameters.
---
>         startprob, 't' for transmat, 'm' for means and 'c' for covars.
>         Defaults to all parameters.
103a91,116
>     Attributes
>     ----------
>     n_features : int
>         Dimensionality of the Gaussian emissions.
>
>     monitor\_ : ConvergenceMonitor
>         Monitor object used to check the convergence of EM.
>
>     transmat\_ : array, shape (n_components, n_components)
>         Matrix of transition probabilities between states.
>
>     startprob\_ : array, shape (n_components, )
>         Initial state occupation distribution.
>
>     means\_ : array, shape (n_components, n_features)
>         Mean parameters for each state.
>
>     covars\_ : array
>         Covariance parameters for each state.
>
>         The shape depends on ``covariance_type``::
>
>             (n_components, )                        if 'spherical',
>             (n_features, n_features)                if 'tied',
>             (n_components, n_features)              if 'diag',
>             (n_components, n_features, n_features)  if 'full'
111,115d123
<
<
<     See Also
<     --------
<     GMM : Gaussian mixture model
117,120c125,128
<
<     def __init__(self, n_components=1, covariance_type='diag', startprob=None,
<                  transmat=None, startprob_prior=None, transmat_prior=None,
<                  algorithm="viterbi", means_prior=None, means_weight=0,
---
>     def __init__(self, n_components=1, covariance_type='diag',
>                  min_covar=1e-3,
>                  startprob_prior=1.0, transmat_prior=1.0,
>                  means_prior=0, means_weight=0,
122,125c130,133
<                  random_state=None, n_iter=10, thresh=1e-2,
<                  params=string.ascii_letters,
<                  init_params=string.ascii_letters):
<         _BaseHMM.__init__(self, n_components, startprob, transmat,
---
>                  algorithm="viterbi", random_state=None,
>                  n_iter=10, tol=1e-2, verbose=False,
>                  params="stmc", init_params="stmc"):
>         _BaseHMM.__init__(self, n_components,
129c137
<                           thresh=thresh, params=params,
---
>                           tol=tol, params=params, verbose=verbose,
132,135c140,141
<         self._covariance_type = covariance_type
<         if covariance_type not in ['spherical', 'tied', 'diag', 'full']:
<             raise ValueError('bad covariance_type')
<
---
>         self.covariance_type = covariance_type
>         self.min_covar = min_covar
138d143
<
142,164d146
<     @property
<     def covariance_type(self):
<         """Covariance type of the model.
<
<         Must be one of 'spherical', 'tied', 'diag', 'full'.
<         """
<         return self._covariance_type
<
<     def _get_means(self):
<         """Mean parameters for each state."""
<         return self._means_
<
<     def _set_means(self, means):
<         means = np.asarray(means)
<         if (hasattr(self, 'n_features')
<                 and means.shape != (self.n_components, self.n_features)):
<             raise ValueError('means must have shape '
<                              '(n_components, n_features)')
<         self._means_ = means.copy()
<         self.n_features = self._means_.shape[1]
<
<     means_ = property(_get_means, _set_means)
<
167c149
<         if self._covariance_type == 'full':
---
>         if self.covariance_type == 'full':
169,174c151,157
<         elif self._covariance_type == 'diag':
<             return [np.diag(cov) for cov in self._covars_]
<         elif self._covariance_type == 'tied':
<             return [self._covars_] * self.n_components
<         elif self._covariance_type == 'spherical':
<             return [np.eye(self.n_features) * f for f in self._covars_]
---
>         elif self.covariance_type == 'diag':
>             return np.array([np.diag(cov) for cov in self._covars_])
>         elif self.covariance_type == 'tied':
>             return np.array([self._covars_] * self.n_components)
>         elif self.covariance_type == 'spherical':
>             return np.array(
>                 [np.eye(self.n_features) * cov for cov in self._covars_])
177,179c160
<         covars = np.asarray(covars)
<         _validate_covars(covars, self._covariance_type, self.n_components)
<         self._covars_ = covars.copy()
---
>         self._covars_ = np.asarray(covars).copy()
183,185c164,165
<     def _compute_log_likelihood(self, obs):
<         return log_multivariate_normal_density(
<             obs, self._means_, self._covars_, self._covariance_type)
---
>     def _check(self):
>         super(GaussianHMM, self)._check()
187,193c167,168
<     def _generate_sample_from_state(self, state, random_state=None):
<         if self._covariance_type == 'tied':
<             cv = self._covars_
<         else:
<             cv = self._covars_[state]
<         return sample_gaussian(self._means_[state], cv, self._covariance_type,
<                                random_state=random_state)
---
>         self.means_ = np.asarray(self.means_)
>         self.n_features = self.means_.shape[1]
195,196c170,172
<     def _init(self, obs, params='stmc'):
<         super(GaussianHMM, self)._init(obs, params=params)
---
>         if self.covariance_type not in COVARIANCE_TYPES:
>             raise ValueError('covariance_type must be one of {0}'
>                              .format(COVARIANCE_TYPES))
198,202c174,175
<         if (hasattr(self, 'n_features')
<                 and self.n_features != obs[0].shape[1]):
<             raise ValueError('Unexpected number of dimensions, got %s but '
<                              'expected %s' % (obs[0].shape[1],
<                                               self.n_features))
---
>         _validate_covars(self._covars_, self.covariance_type,
>                          self.n_components)
204c177,183
<         self.n_features = obs[0].shape[1]
---
>     def _init(self, X, lengths=None):
>         super(GaussianHMM, self)._init(X, lengths=lengths)
>
>         _, n_features = X.shape
>         if hasattr(self, 'n_features') and self.n_features != n_features:
>             raise ValueError('Unexpected number of dimensions, got %s but '
>                              'expected %s' % (n_features, self.n_features))
206,210c185,192
<         if 'm' in params:
<             self._means_ = cluster.KMeans(
<                 n_clusters=self.n_components).fit(obs[0]).cluster_centers_
<         if 'c' in params:
<             cv = np.cov(obs[0].T)
---
>         self.n_features = n_features
>         if 'm' in self.init_params or not hasattr(self, "means_"):
>             kmeans = cluster.KMeans(n_clusters=self.n_components,
>                                     random_state=self.random_state)
>             kmeans.fit(X)
>             self.means_ = kmeans.cluster_centers_
>         if 'c' in self.init_params or not hasattr(self, "covars_"):
>             cv = np.cov(X.T) + self.min_covar * np.eye(X.shape[1])
214,215c196,208
<                 cv, self._covariance_type, self.n_components)
<             self._covars_[self._covars_ == 0] = 1e-5
---
>                 cv, self.covariance_type, self.n_components).copy()
>
>     def _compute_log_likelihood(self, X):
>         return log_multivariate_normal_density(
>             X, self.means_, self._covars_, self.covariance_type)
>
>     def _generate_sample_from_state(self, state, random_state=None):
>         if self.covariance_type == 'tied':
>             cv = self._covars_
>         else:
>             cv = self._covars_[state]
>         return sample_gaussian(self.means_[state], cv, self.covariance_type,
>                                random_state=random_state)
222c215
<         if self._covariance_type in ('tied', 'full'):
---
>         if self.covariance_type in ('tied', 'full'):
224c217
<                                           self.n_features))
---
>                                            self.n_features))
228,229c221
<                                           posteriors, fwdlattice, bwdlattice,
<                                           params):
---
>                                           posteriors, fwdlattice, bwdlattice):
231,232c223
<             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice,
<             params)
---
>             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice)
234c225
<         if 'm' in params or 'c' in params:
---
>         if 'm' in self.params or 'c' in self.params:
238,239c229,230
<         if 'c' in params:
<             if self._covariance_type in ('spherical', 'diag'):
---
>         if 'c' in self.params:
>             if self.covariance_type in ('spherical', 'diag'):
241,245c232,236
<             elif self._covariance_type in ('tied', 'full'):
<                 for t, o in enumerate(obs):
<                     obsobsT = np.outer(o, o)
<                     for c in range(self.n_components):
<                         stats['obs*obs.T'][c] += posteriors[t, c] * obsobsT
---
>             elif self.covariance_type in ('tied', 'full'):
>                 # posteriors: (nt, nc); obs: (nt, nf); obs: (nt, nf)
>                 # -> (nc, nf, nf)
>                 stats['obs*obs.T'] += np.einsum(
>                     'ij,ik,il->jkl', posteriors, obs, obs)
247,248c238,239
<     def _do_mstep(self, stats, params):
<         super(GaussianHMM, self)._do_mstep(stats, params)
---
>     def _do_mstep(self, stats):
>         super(GaussianHMM, self)._do_mstep(stats)
249a241,245
>         means_prior = self.means_prior
>         means_weight = self.means_weight
>
>         # TODO: find a proper reference for estimates for different
>         #       covariance models.
253,259c249,251
<         if 'm' in params:
<             prior = self.means_prior
<             weight = self.means_weight
<             if prior is None:
<                 weight = 0
<                 prior = 0
<             self._means_ = (weight * prior + stats['obs']) / (weight + denom)
---
>         if 'm' in self.params:
>             self.means_ = ((means_weight * means_prior + stats['obs'])
>                            / (means_weight + denom))
261c253
<         if 'c' in params:
---
>         if 'c' in self.params:
264,273c256
<             if covars_prior is None:
<                 covars_weight = 0
<                 covars_prior = 0
<
<             means_prior = self.means_prior
<             means_weight = self.means_weight
<             if means_prior is None:
<                 means_weight = 0
<                 means_prior = 0
<             meandiff = self._means_ - means_prior
---
>             meandiff = self.means_ - means_prior
275,276c258,259
<             if self._covariance_type in ('spherical', 'diag'):
<                 cv_num = (means_weight * (meandiff) ** 2
---
>             if self.covariance_type in ('spherical', 'diag'):
>                 cv_num = (means_weight * meandiff**2
278,279c261,262
<                           - 2 * self._means_ * stats['obs']
<                           + self._means_ ** 2 * denom)
---
>                           - 2 * self.means_ * stats['obs']
>                           + self.means_**2 * denom)
281,282c264,266
<                 self._covars_ = (covars_prior + cv_num) / np.maximum(cv_den, 1e-5)
<                 if self._covariance_type == 'spherical':
---
>                 self._covars_ = \
>                     (covars_prior + cv_num) / np.maximum(cv_den, 1e-5)
>                 if self.covariance_type == 'spherical':
286,287c270,271
<             elif self._covariance_type in ('tied', 'full'):
<                 cvnum = np.empty((self.n_components, self.n_features,
---
>             elif self.covariance_type in ('tied', 'full'):
>                 cv_num = np.empty((self.n_components, self.n_features,
290c274
<                     obsmean = np.outer(stats['obs'][c], self._means_[c])
---
>                     obsmean = np.outer(stats['obs'][c], self.means_[c])
292,297c276,281
<                     cvnum[c] = (means_weight * np.outer(meandiff[c],
<                                                         meandiff[c])
<                                 + stats['obs*obs.T'][c]
<                                 - obsmean - obsmean.T
<                                 + np.outer(self._means_[c], self._means_[c])
<                                 * stats['post'][c])
---
>                     cv_num[c] = (means_weight * np.outer(meandiff[c],
>                                                          meandiff[c])
>                                  + stats['obs*obs.T'][c]
>                                  - obsmean - obsmean.T
>                                  + np.outer(self.means_[c], self.means_[c])
>                                  * stats['post'][c])
299,300c283,284
<                 if self._covariance_type == 'tied':
<                     self._covars_ = ((covars_prior + cvnum.sum(axis=0)) /
---
>                 if self.covariance_type == 'tied':
>                     self._covars_ = ((covars_prior + cv_num.sum(axis=0)) /
302,303c286,287
<                 elif self._covariance_type == 'full':
<                     self._covars_ = ((covars_prior + cvnum) /
---
>                 elif self.covariance_type == 'full':
>                     self._covars_ = ((covars_prior + cv_num) /
306,329d289
<     def fit(self, obs):
<         """Estimate model parameters.
<
<         An initialization step is performed before entering the EM
<         algorithm. If you want to avoid this step, pass proper
<         ``init_params`` keyword argument to estimator's constructor.
<
<         Parameters
<         ----------
<         obs : list
<             List of array-like observation sequences, each of which
<             has shape (n_i, n_features), where n_i is the length of
<             the i_th observation.
<
<         Notes
<         -----
<         In general, `logprob` should be non-decreasing unless
<         aggressive pruning is used.  Decreasing `logprob` is generally
<         a sign of overfitting (e.g. the covariance parameter on one or
<         more components becomminging too small).  You can fix this by getting
<         more training data, or increasing covars_prior.
<         """
<         return super(GaussianHMM, self).fit(obs)
<
334c294
<     Attributes
---
>     Parameters
336,337d295
<     n_components : int
<         Number of states in the model.
339,340c297,298
<     n_symbols : int
<         Number of possible symbols emitted by the model (in the observations).
---
>     n_components : int
>         Number of states.
342,343c300,301
<     transmat : array, shape (`n_components`, `n_components`)
<         Matrix of transition probabilities between states.
---
>     startprob_prior : array, shape (n_components, )
>         Initial state occupation prior distribution.
345,346c303,304
<     startprob : array, shape ('n_components`,)
<         Initial state occupation distribution.
---
>     transmat_prior : array, shape (n_components, n_components)
>         Matrix of prior transition probabilities between states.
348,349c306,308
<     emissionprob : array, shape ('n_components`, 'n_symbols`)
<         Probability of emitting a given symbol when in each state.
---
>     algorithm : string
>         Decoder algorithm. Must be one of "viterbi" or "map".
>         Defaults to "viterbi".
351,352c310,311
<     random_state: RandomState or an int seed (0 by default)
<         A random number generator instance
---
>     random_state: RandomState or an int seed
>         A random number generator instance.
355c314
<         Number of iterations to perform.
---
>         Maximum number of iterations to perform.
357,358c316,323
<     thresh : float, optional
<         Convergence threshold.
---
>     tol : float, optional
>         Convergence threshold. EM will stop if the gain in log-likelihood
>         is below this value.
>
>     verbose : bool, optional
>         When ``True`` per-iteration convergence reports are printed
>         to :data:`sys.stderr`. You can diagnose convergence via the
>         :attr:`monitor_` attribute.
363c328
<         't' for transmat, 'e' for emmissionprob.
---
>         't' for transmat, 'e' for emissionprob.
369c334
<         startprob, 't' for transmat, 'e' for emmissionprob.
---
>         startprob, 't' for transmat, 'e' for emissionprob.
371a337,353
>     Attributes
>     ----------
>     n_features : int
>         Number of possible symbols emitted by the model (in the samples).
>
>     monitor\_ : ConvergenceMonitor
>         Monitor object used to check the convergence of EM.
>
>     transmat\_ : array, shape (n_components, n_components)
>         Matrix of transition probabilities between states.
>
>     startprob\_ : array, shape (n_components, )
>         Initial state occupation distribution.
>
>     emissionprob\_ : array, shape (n_components, n_features)
>         Probability of emitting a given symbol when in each state.
>
378,381d359
<
<     See Also
<     --------
<     GaussianHMM : HMM with Gaussian emissions
384,385c362,363
<     def __init__(self, n_components=1, startprob=None, transmat=None,
<                  startprob_prior=None, transmat_prior=None,
---
>     def __init__(self, n_components=1,
>                  startprob_prior=1.0, transmat_prior=1.0,
387,396c365,367
<                  n_iter=10, thresh=1e-2, params=string.ascii_letters,
<                  init_params=string.ascii_letters):
<         """Create a hidden Markov model with multinomial emissions.
<
<         Parameters
<         ----------
<         n_components : int
<             Number of states.
<         """
<         _BaseHMM.__init__(self, n_components, startprob, transmat,
---
>                  n_iter=10, tol=1e-2, verbose=False,
>                  params="ste", init_params="ste"):
>         _BaseHMM.__init__(self, n_components,
401,404c372,378
<                           n_iter=n_iter,
<                           thresh=thresh,
<                           params=params,
<                           init_params=init_params)
---
>                           n_iter=n_iter, tol=tol, verbose=verbose,
>                           params=params, init_params=init_params)
>
>     def _init(self, X, lengths=None):
>         if not self._check_input_symbols(X):
>             raise ValueError("expected a sample from "
>                              "a Multinomial distribution.")
406,425c380,381
<     def _get_emissionprob(self):
<         """Emission probability distribution for each state."""
<         return np.exp(self._log_emissionprob)
<
<     def _set_emissionprob(self, emissionprob):
<         emissionprob = np.asarray(emissionprob)
<         if hasattr(self, 'n_symbols') and \
<                 emissionprob.shape != (self.n_components, self.n_symbols):
<             raise ValueError('emissionprob must have shape '
<                              '(n_components, n_symbols)')
<
<         # check if there exists a component whose value is exactly zero
<         # if so, add a small number and re-normalize
<         if not np.alltrue(emissionprob):
<             normalize(emissionprob)
<
<         self._log_emissionprob = np.log(emissionprob)
<         underflow_idx = np.isnan(self._log_emissionprob)
<         self._log_emissionprob[underflow_idx] = NEGINF
<         self.n_symbols = self._log_emissionprob.shape[1]
---
>         super(MultinomialHMM, self)._init(X, lengths=lengths)
>         self.random_state = check_random_state(self.random_state)
427c383,402
<     emissionprob_ = property(_get_emissionprob, _set_emissionprob)
---
>         if 'e' in self.init_params:
>             if not hasattr(self, "n_features"):
>                 symbols = set()
>                 for i, j in iter_from_X_lengths(X, lengths):
>                     symbols |= set(X[i:j].flatten())
>                 self.n_features = len(symbols)
>             self.emissionprob_ = self.random_state \
>                 .rand(self.n_components, self.n_features)
>             normalize(self.emissionprob_, axis=1)
>
>     def _check(self):
>         super(MultinomialHMM, self)._check()
>
>         self.emissionprob_ = np.atleast_2d(self.emissionprob_)
>         n_features = getattr(self, "n_features", self.emissionprob_.shape[1])
>         if self.emissionprob_.shape != (self.n_components, n_features):
>             raise ValueError(
>                 "emissionprob_ must have shape (n_components, n_features)")
>         else:
>             self.n_features = n_features
429,430c404,405
<     def _compute_log_likelihood(self, obs):
<         return self._log_emissionprob[:, obs].T
---
>     def _compute_log_likelihood(self, X):
>         return np.log(self.emissionprob_)[:, np.concatenate(X)].T
435,451c410
<         rand = random_state.rand()
<         symbol = (cdf > rand).argmax()
<         return symbol
<
<     def _init(self, obs, params='ste'):
<         super(MultinomialHMM, self)._init(obs, params=params)
<         self.random_state = check_random_state(self.random_state)
<
<         if 'e' in params:
<             if not hasattr(self, 'n_symbols'):
<                 symbols = set()
<                 for o in obs:
<                     symbols = symbols.union(set(o))
<                 self.n_symbols = len(symbols)
<             emissionprob = normalize(self.random_state.rand(self.n_components,
<                                                             self.n_symbols), 1)
<             self.emissionprob_ = emissionprob
---
>         return [(cdf > random_state.rand()).argmax()]
455c414
<         stats['obs'] = np.zeros((self.n_components, self.n_symbols))
---
>         stats['obs'] = np.zeros((self.n_components, self.n_features))
458,460c417,418
<     def _accumulate_sufficient_statistics(self, stats, obs, framelogprob,
<                                           posteriors, fwdlattice, bwdlattice,
<                                           params):
---
>     def _accumulate_sufficient_statistics(self, stats, X, framelogprob,
>                                           posteriors, fwdlattice, bwdlattice):
462,465c420,422
<             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice,
<             params)
<         if 'e' in params:
<             for t, symbol in enumerate(obs):
---
>             stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)
>         if 'e' in self.params:
>             for t, symbol in enumerate(np.concatenate(X)):
468,470c425,427
<     def _do_mstep(self, stats, params):
<         super(MultinomialHMM, self)._do_mstep(stats, params)
<         if 'e' in params:
---
>     def _do_mstep(self, stats):
>         super(MultinomialHMM, self)._do_mstep(stats)
>         if 'e' in self.params:
474,475c431,432
<     def _check_input_symbols(self, obs):
<         """Check if ``obs`` is a sample from a Multinomial distribution.
---
>     def _check_input_symbols(self, X):
>         """Check if ``X`` is a sample from a Multinomial distribution.
477,479c434,436
<         That is ``obs`` should be an array of non-negative integers from
<         range ``[min(obs), max(obs)]``, such that each integer from the range
<         occurs in ``obs`` at least once.
---
>         That is ``X`` should be an array of non-negative integers from
>         range ``[min(X), max(X)]``, such that each integer from the range
>         occurs in ``X`` at least once.
484c441
<         symbols = np.concatenate(obs)
---
>         symbols = np.concatenate(X)
487c444
<             np.any(symbols < 0)):         # contains negative integers
---
>             (symbols < 0).any()):         # contains negative integers
493,494d449
<     def fit(self, obs, **kwargs):
<         """Estimate model parameters.
496,509c451,452
<         An initialization step is performed before entering the EM
<         algorithm. If you want to avoid this step, pass proper
<         ``init_params`` keyword argument to estimator's constructor.
<
<         Parameters
<         ----------
<         obs : list
<             List of array-like observation sequences. Each observation
<             sequence should consist of two or more integers from
<             range ``[0, n_symbols - 1]``.
<         """
<         if not self._check_input_symbols(obs):
<             raise ValueError("expected a sample from "
<                              "a Multinomial distribution.")
---
> class GMMHMM(_BaseHMM):
>     """Hidden Markov Model with Gaussian mixture emissions.
511c454,457
<         return _BaseHMM.fit(self, obs, **kwargs)
---
>     Parameters
>     ----------
>     n_components : int
>         Number of states in the model.
512a459,460
>     n_mix : int
>         Number of states in the GMM.
514,515c462,498
< class GMMHMM(_BaseHMM):
<     """Hidden Markov Model with Gaussin mixture emissions
---
>     covariance_type : string
>         String describing the type of covariance parameters to
>         use.  Must be one of
>
>         * "spherical" --- each state uses a single variance value that
>           applies to all features;
>         * "diag" --- each state uses a diagonal covariance matrix;
>         * "full" --- each state uses a full (i.e. unrestricted)
>           covariance matrix;
>         * "tied" --- all states use **the same** full covariance matrix.
>
>         Defaults to "diag".
>
>     startprob_prior : array, shape (n_components, )
>         Initial state occupation prior distribution.
>
>     transmat_prior : array, shape (n_components, n_components)
>         Matrix of prior transition probabilities between states.
>
>     algorithm : string
>         Decoder algorithm. Must be one of "viterbi" or "map".
>         Defaults to "viterbi".
>
>     random_state: RandomState or an int seed
>         A random number generator instance.
>
>     n_iter : int, optional
>         Maximum number of iterations to perform.
>
>     tol : float, optional
>         Convergence threshold. EM will stop if the gain in log-likelihood
>         is below this value.
>
>     verbose : bool, optional
>         When ``True`` per-iteration convergence reports are printed
>         to :data:`sys.stderr`. You can diagnose convergence via the
>         :attr:`monitor_` attribute.
517,518d499
<     Attributes
<     ----------
531,535c512,515
<     n_components : int
<         Number of states in the model.
<
<     transmat : array, shape (`n_components`, `n_components`)
<         Matrix of transition probabilities between states.
---
>     Attributes
>     ----------
>     monitor\_ : ConvergenceMonitor
>         Monitor object used to check the convergence of EM.
537c517
<     startprob : array, shape ('n_components`,)
---
>     startprob\_ : array, shape (n_components, )
540,547c520,521
<     gmms : array of GMM objects, length `n_components`
<         GMM emission distributions for each state.
<
<     random_state : RandomState or an int seed (0 by default)
<         A random number generator instance
<
<     n_iter : int, optional
<         Number of iterations to perform.
---
>     transmat\_ : array, shape (n_components, n_components)
>         Matrix of transition probabilities between states.
549,550c523,524
<     thresh : float, optional
<         Convergence threshold.
---
>     gmms\_ : list of GMM objects, length n_components
>         GMM emission distributions for each state.
558,561d531
<
<     See Also
<     --------
<     GaussianHMM : HMM with Gaussian emissions
564,577c534,540
<     def __init__(self, n_components=1, n_mix=1, startprob=None, transmat=None,
<                  startprob_prior=None, transmat_prior=None,
<                  algorithm="viterbi", gmms=None, covariance_type='diag',
<                  covars_prior=1e-2, random_state=None, n_iter=10, thresh=1e-2,
<                  params=string.ascii_letters,
<                  init_params=string.ascii_letters):
<         """Create a hidden Markov model with GMM emissions.
<
<         Parameters
<         ----------
<         n_components : int
<             Number of states.
<         """
<         _BaseHMM.__init__(self, n_components, startprob, transmat,
---
>     def __init__(self, n_components=1, n_mix=1,
>                  startprob_prior=1.0, transmat_prior=1.0,
>                  covariance_type='diag', covars_prior=1e-2,
>                  algorithm="viterbi", random_state=None,
>                  n_iter=10, tol=1e-2, verbose=False,
>                  params="stmcw", init_params="stmcw"):
>         _BaseHMM.__init__(self, n_components,
580,585c543,551
<                           algorithm=algorithm,
<                           random_state=random_state,
<                           n_iter=n_iter,
<                           thresh=thresh,
<                           params=params,
<                           init_params=init_params)
---
>                           algorithm=algorithm, random_state=random_state,
>                           n_iter=n_iter, tol=tol, verbose=verbose,
>                           params=params, init_params=init_params)
>
>         if covariance_type != "diag":
>             warnings.warn("Fitting a GMMHMM with {0!r} covariance type "
>                           "is broken in 0.2.0. Please update to 0.2.1 once "
>                           "it's available.".format(covariance_type),
>                           UserWarning)
590c556
<         self._covariance_type = covariance_type
---
>         self.covariance_type = covariance_type
592,606c558,565
<         self.gmms = gmms
<         if gmms is None:
<             gmms = []
<             for x in range(self.n_components):
<                 if covariance_type is None:
<                     g = GMM(n_mix)
<                 else:
<                     g = GMM(n_mix, covariance_type=covariance_type)
<                 gmms.append(g)
<         self.gmms_ = gmms
<
<     # Read-only properties.
<     @property
<     def covariance_type(self):
<         """Covariance type of the model.
---
>         self.gmms_ = []
>         for x in range(self.n_components):
>             if covariance_type is None:
>                 gmm = GMM(n_mix, random_state=self.random_state)
>             else:
>                 gmm = GMM(n_mix, covariance_type=covariance_type,
>                         random_state=self.random_state)
>             self.gmms_.append(gmm)
608,610c567,568
<         Must be one of 'spherical', 'tied', 'diag', 'full'.
<         """
<         return self._covariance_type
---
>     def _init(self, X, lengths=None):
>         super(GMMHMM, self)._init(X, lengths=lengths)
612,613c570,575
<     def _compute_log_likelihood(self, obs):
<         return np.array([g.score(obs) for g in self.gmms_]).T
---
>         for g in self.gmms_:
>             g.set_params(init_params=self.init_params, n_iter=0)
>             g.fit(X)
>
>     def _compute_log_likelihood(self, X):
>         return np.array([g.score(X) for g in self.gmms_]).T
618,625d579
<     def _init(self, obs, params='stwmc'):
<         super(GMMHMM, self)._init(obs, params=params)
<
<         allobs = np.concatenate(obs, 0)
<         for g in self.gmms_:
<             g.set_params(init_params=params, n_iter=0)
<             g.fit(allobs)
<
633,635c587,588
<     def _accumulate_sufficient_statistics(self, stats, obs, framelogprob,
<                                           posteriors, fwdlattice, bwdlattice,
<                                           params):
---
>     def _accumulate_sufficient_statistics(self, stats, X, framelogprob,
>                                           posteriors, fwdlattice, bwdlattice):
637,638c590
<             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice,
<             params)
---
>             stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)
641,643c593,595
<             _, lgmm_posteriors = g.score_samples(obs)
<             lgmm_posteriors += np.log(posteriors[:, state][:, np.newaxis]
<                                       + np.finfo(np.float).eps)
---
>             lgmm_posteriors = (np.log(g.predict_proba(X))
>                                + np.log(posteriors[:, state][:, np.newaxis]
>                                         + np.finfo(np.float).eps))
645c597
<             tmp_gmm = GMM(g.n_components, covariance_type=g.covariance_type)
---
>
646a599
>             tmp_gmm = GMM(g.n_components, covariance_type=g.covariance_type)
651c604
<             norm = tmp_gmm._do_mstep(obs, gmm_posteriors, params)
---
>             norm = tmp_gmm._do_mstep(X, gmm_posteriors, self.params)
657c610
<             if 'm' in params:
---
>             if 'm' in self.params:
659c612
<             if 'c' in params:
---
>             if 'c' in self.params:
664c617
<                     shape = np.ones(tmp_gmm.covars_.ndim)
---
>                     shape = np.ones(tmp_gmm.covars_.ndim, dtype=np.int)
667c620,624
<                     stats['covars'][state] += tmp_gmm.covars_ * cvnorm
---
>                     stats['covars'][state] += (tmp_gmm.covars_
>                                                + tmp_gmm.means_**2) * cvnorm
>
>     def _do_mstep(self, stats):
>         super(GMMHMM, self)._do_mstep(stats)
669,670d625
<     def _do_mstep(self, stats, params):
<         super(GMMHMM, self)._do_mstep(stats, params)
676,678c631,634
<             if 'w' in params:
<                 g.weights_ = normalize(norm.copy())
<             if 'm' in params:
---
>             if 'w' in self.params:
>                 g.weights_ = norm.copy()
>                 normalize(g.weights_)
>             if 'm' in self.params:
680c636
<             if 'c' in params:
---
>             if 'c' in self.params:
687c643
<                     shape = np.ones(g.covars_.ndim)
---
>                     shape = np.ones(g.covars_.ndim, dtype=np.int)
690c646
<                     if (g.covariance_type in ['spherical', 'diag']):
---
>                     if g.covariance_type in ['spherical', 'diag']:
692c648
<                                      self.covars_prior) / cvnorm
---
>                                      self.covars_prior) / cvnorm - g.means_**2
697c653
<                                      / cvnorm)
---
>                                      / cvnorm) - g.means_**2
